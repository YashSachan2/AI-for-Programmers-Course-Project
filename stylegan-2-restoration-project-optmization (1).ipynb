{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/yohan-pg/robust-unsupervised.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T18:58:43.460725Z","iopub.execute_input":"2025-11-22T18:58:43.460907Z","iopub.status.idle":"2025-11-22T18:58:44.898737Z","shell.execute_reply.started":"2025-11-22T18:58:43.460889Z","shell.execute_reply":"2025-11-22T18:58:44.897975Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'robust-unsupervised'...\nremote: Enumerating objects: 204, done.\u001b[K\nremote: Counting objects: 100% (10/10), done.\u001b[K\nremote: Compressing objects: 100% (5/5), done.\u001b[K\nremote: Total 204 (delta 5), reused 5 (delta 5), pack-reused 194 (from 1)\u001b[K\nReceiving objects: 100% (204/204), 34.85 MiB | 50.19 MiB/s, done.\nResolving deltas: 100% (80/80), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# segmentation + transforms\nimport torchvision\nfrom torchvision import transforms as T\nfrom torchvision.models.segmentation import deeplabv3_resnet50\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T18:58:48.283443Z","iopub.execute_input":"2025-11-22T18:58:48.284186Z","iopub.status.idle":"2025-11-22T18:58:54.951397Z","shell.execute_reply.started":"2025-11-22T18:58:48.284113Z","shell.execute_reply":"2025-11-22T18:58:54.950840Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"%cd robust-unsupervised","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T18:58:58.740213Z","iopub.execute_input":"2025-11-22T18:58:58.740749Z","iopub.status.idle":"2025-11-22T18:58:58.747806Z","shell.execute_reply.started":"2025-11-22T18:58:58.740724Z","shell.execute_reply":"2025-11-22T18:58:58.747183Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/robust-unsupervised\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install tyro \"git+https://github.com/jwblangley/pytorch-fid.git\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T18:59:03.070464Z","iopub.execute_input":"2025-11-22T18:59:03.070733Z","iopub.status.idle":"2025-11-22T19:00:17.448782Z","shell.execute_reply.started":"2025-11-22T18:59:03.070711Z","shell.execute_reply":"2025-11-22T19:00:17.447662Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/jwblangley/pytorch-fid.git\n  Cloning https://github.com/jwblangley/pytorch-fid.git to /tmp/pip-req-build-81szk0sf\n  Running command git clone --filter=blob:none --quiet https://github.com/jwblangley/pytorch-fid.git /tmp/pip-req-build-81szk0sf\n  Resolved https://github.com/jwblangley/pytorch-fid.git to commit 3d604a25516746c3a4a5548c8610e99010b2c819\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting tyro\n  Downloading tyro-0.9.35-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro) (0.16)\nRequirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro) (14.0.0)\nCollecting shtab>=1.5.6 (from tyro)\n  Downloading shtab-1.8.0-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro) (4.4.4)\nRequirement already satisfied: typing-extensions>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from tyro) (4.14.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pytorch-fid==0.2.1) (1.26.4)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pytorch-fid==0.2.1) (11.2.1)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pytorch-fid==0.2.1) (1.15.3)\nRequirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from pytorch-fid==0.2.1) (2.6.0+cu124)\nRequirement already satisfied: torchvision>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from pytorch-fid==0.2.1) (0.21.0+cu124)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro) (2.19.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid==0.2.1) (3.18.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid==0.2.1) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid==0.2.1) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid==0.2.1) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.0.1->pytorch-fid==0.2.1)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.0.1->pytorch-fid==0.2.1)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.0.1->pytorch-fid==0.2.1)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.0.1->pytorch-fid==0.2.1)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.0.1->pytorch-fid==0.2.1)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.0.1->pytorch-fid==0.2.1)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.0.1->pytorch-fid==0.2.1)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.0.1->pytorch-fid==0.2.1)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.0.1->pytorch-fid==0.2.1)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid==0.2.1) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid==0.2.1) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid==0.2.1) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.0.1->pytorch-fid==0.2.1)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid==0.2.1) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid==0.2.1) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.1->pytorch-fid==0.2.1) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->pytorch-fid==0.2.1) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->pytorch-fid==0.2.1) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->pytorch-fid==0.2.1) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->pytorch-fid==0.2.1) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->pytorch-fid==0.2.1) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->pytorch-fid==0.2.1) (2.4.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro) (0.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.1->pytorch-fid==0.2.1) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->pytorch-fid==0.2.1) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->pytorch-fid==0.2.1) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->pytorch-fid==0.2.1) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->pytorch-fid==0.2.1) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->pytorch-fid==0.2.1) (2024.2.0)\nDownloading tyro-0.9.35-py3-none-any.whl (132 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading shtab-1.8.0-py3-none-any.whl (14 kB)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pytorch-fid\n  Building wheel for pytorch-fid (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pytorch-fid: filename=pytorch_fid-0.2.1-py3-none-any.whl size=14811 sha256=b55b93dd0299b8e9a78920319588d6f4ebd25b81f889344ebc9b52ba0cc350cd\n  Stored in directory: /tmp/pip-ephem-wheel-cache-o1jnlk20/wheels/8f/47/c7/95c4bdd845ec42b55ef9e4fe7b594c8304cfa618566f25af22\nSuccessfully built pytorch-fid\nInstalling collected packages: shtab, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tyro, nvidia-cusolver-cu12, pytorch-fid\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch-fid-0.2.1 shtab-1.8.0 tyro-0.9.35\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!wget https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl -O pretrained_networks/ffhq.pkl\n\n# This cell replaces the StyleGAN2 download command with one for StyleGAN3.\n# We are using the 'stylegan3-r' (rotation equivariant) variant trained on FFHQ.\n# !wget https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-ffhq-1024x1024.pkl -O pretrained_networks/ffhq.pkl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T19:00:19.728576Z","iopub.execute_input":"2025-11-22T19:00:19.729153Z","iopub.status.idle":"2025-11-22T19:00:32.079035Z","shell.execute_reply.started":"2025-11-22T19:00:19.729103Z","shell.execute_reply":"2025-11-22T19:00:32.078250Z"}},"outputs":[{"name":"stdout","text":"--2025-11-22 19:00:19--  https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl\nResolving nvlabs-fi-cdn.nvidia.com (nvlabs-fi-cdn.nvidia.com)... 18.244.202.81, 18.244.202.50, 18.244.202.42, ...\nConnecting to nvlabs-fi-cdn.nvidia.com (nvlabs-fi-cdn.nvidia.com)|18.244.202.81|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 381624121 (364M) [binary/octet-stream]\nSaving to: ‘pretrained_networks/ffhq.pkl’\n\npretrained_networks 100%[===================>] 363.94M  34.0MB/s    in 11s     \n\n2025-11-22 19:00:31 (31.7 MB/s) - ‘pretrained_networks/ffhq.pkl’ saved [381624121/381624121]\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install lpips","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T19:00:36.380405Z","iopub.execute_input":"2025-11-22T19:00:36.380708Z","iopub.status.idle":"2025-11-22T19:00:40.838686Z","shell.execute_reply.started":"2025-11-22T19:00:36.380679Z","shell.execute_reply":"2025-11-22T19:00:40.837928Z"}},"outputs":[{"name":"stdout","text":"Collecting lpips\n  Downloading lpips-0.1.4-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from lpips) (2.6.0+cu124)\nRequirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from lpips) (0.21.0+cu124)\nRequirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.11/dist-packages (from lpips) (1.26.4)\nRequirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from lpips) (1.15.3)\nRequirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.11/dist-packages (from lpips) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.3->lpips) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.3->lpips) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.3->lpips) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.3->lpips) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.3->lpips) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.3->lpips) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->lpips) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->lpips) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->lpips) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->lpips) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->lpips) (2025.5.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->lpips) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->lpips) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->lpips) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->lpips) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->lpips) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->lpips) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->lpips) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->lpips) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->lpips) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->lpips) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->lpips) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->lpips) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->lpips) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->lpips) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->lpips) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=0.4.0->lpips) (1.3.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision>=0.2.1->lpips) (11.2.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=0.4.0->lpips) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.3->lpips) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.3->lpips) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.3->lpips) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.14.3->lpips) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.14.3->lpips) (2024.2.0)\nDownloading lpips-0.1.4-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: lpips\nSuccessfully installed lpips-0.1.4\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"run_py_content = r\"\"\"\n# ============================================================\n# === HYBRID OPTIMIZER VERSION OF run.py (READY FOR KAGGLE) ==\n# ============================================================\n\nfrom cli import parse_config\nimport glob, os, datetime, tqdm\nimport benchmark\nfrom benchmark import Task, Degradation\nfrom robust_unsupervised import *\nimport torch\nfrom torch.optim import Adam, RMSprop, AdamW, LBFGS\n\nconfig = parse_config()\nbenchmark.config.resolution = config.resolution\n\nprint(config.name)\ntimestamp = datetime.datetime.now().isoformat(timespec=\"seconds\").replace(\":\", \"\")\n\nG = open_generator(config.pkl_path)\nloss_fn = MultiscaleLPIPS()\n\n\n# ============================================================\n# HYBRID OPTIMIZER SCHEDULE\n# ============================================================\ndef get_optimizer_for_epoch(variable, epoch):\n    if epoch < 30:\n        return Adam(variable.parameters(), lr=0.008)\n    elif epoch < 60:\n        return RMSprop(variable.parameters(), lr=0.006)\n    elif epoch < 90:\n        return AdamW(variable.parameters(), lr=0.004, weight_decay=1e-4)\n    else:\n        return LBFGS(variable.parameters(), lr=0.1, max_iter=1, history_size=5)\n\n\n# ============================================================\n# SEGMENTATION HOOK (OPTIONAL)\n# ============================================================\ndef apply_segmentation_mask(image):\n    return image  # placeholder\n\n\n# ============================================================\n# MODIFIED TRAINING PHASE WITH OPTIMIZER SWITCHING\n# ============================================================\ndef run_phase(label: str, variable: Variable, base_lr: float):\n\n    try:\n        for epoch in tqdm.tqdm(range(150), desc=f\"{label}\"):\n\n            x = variable.to_image()\n            x = apply_segmentation_mask(x)\n\n            loss = loss_fn(degradation.degrade_prediction, x, target, degradation.mask).mean()\n\n            optimizer = get_optimizer_for_epoch(variable, epoch)\n            optimizer.zero_grad()\n\n            if isinstance(optimizer, LBFGS):\n                def closure():\n                    optimizer.zero_grad()\n                    y = variable.to_image()\n                    y = apply_segmentation_mask(y)\n                    l = loss_fn(degradation.degrade_prediction, y, target, degradation.mask).mean()\n                    l.backward()\n                    return l\n                optimizer.step(closure)\n            else:\n                loss.backward()\n                optimizer.step()\n\n    except KeyboardInterrupt:\n        pass\n\n    suffix = \"_\" + label\n    pred = resize_for_logging(variable.to_image(), config.resolution)\n\n    approx_degraded_pred = degradation.degrade_prediction(pred)\n    degraded_pred = degradation.degrade_ground_truth(pred)\n\n    save_image(pred, f\"pred{suffix}.png\")\n    save_image(degraded_pred, f\"degraded_pred{suffix}.png\")\n    save_image(torch.cat([approx_degraded_pred, degraded_pred]),\n               f\"degradation_approximation{suffix}.jpg\")\n\n    save_image(torch.cat([\n        ground_truth,\n        resize_for_logging(target, config.resolution),\n        resize_for_logging(degraded_pred, config.resolution),\n        pred\n    ]), f\"side_by_side{suffix}.jpg\")\n\n    save_image(torch.cat([resize_for_logging(target, config.resolution), pred]),\n               f\"result{suffix}.jpg\")\n\n    save_image(torch.cat([target, degraded_pred, (target - degraded_pred).abs()]),\n               f\"fidelity{suffix}.jpg\")\n\n    save_image(torch.cat([ground_truth, pred, (ground_truth - pred).abs()]),\n               f\"accuracy{suffix}.jpg\")\n\n\n# ============================================================\n# MAIN LOOP\n# ============================================================\nif __name__ == '__main__':\n\n    if config.tasks == \"single\":\n        tasks = benchmark.single_tasks\n    elif config.tasks == \"composed\":\n        tasks = benchmark.composed_tasks\n    elif config.tasks == \"all\":\n        tasks = benchmark.all_tasks\n    elif config.tasks == \"custom\":\n        class YourDegradation:\n            def degrade_ground_truth(self, x): raise NotImplementedError\n            def degrade_prediction(self, x):  raise NotImplementedError\n        tasks = [Task(constructor=YourDegradation,\n                      name=\"your_degradation\",\n                      category=\"single\",\n                      level=\"M\")]\n    else:\n        raise Exception(\"Invalid task name\")\n\n    for task in tasks:\n        experiment_path = f\"out/{config.name}/{timestamp}/{task.category}/{task.name}/{task.level}/\"\n\n        image_paths = sorted([\n            os.path.abspath(p)\n            for p in glob.glob(config.dataset_path + \"/**/*.png\", recursive=True)\n            + glob.glob(config.dataset_path + \"/**/*.jpg\", recursive=True)\n            + glob.glob(config.dataset_path + \"/**/*.jpeg\", recursive=True)\n            + glob.glob(config.dataset_path + \"/**/*.tif\", recursive=True)\n        ])\n\n        assert len(image_paths) > 0, \"No images found!\"\n\n        with directory(experiment_path):\n            print(experiment_path)\n            print(os.path.abspath(config.dataset_path))\n\n            for j, image_path in enumerate(image_paths):\n                with directory(f\"inversions/{j:04d}\"):\n\n                    print(f\"- {j:04d}\")\n\n                    ground_truth = open_image(image_path, config.resolution)\n                    degradation = task.init_degradation()\n\n                    save_image(ground_truth, f\"ground_truth.png\")\n                    target = degradation.degrade_ground_truth(ground_truth)\n                    save_image(target, f\"target.png\")\n\n                    W_variable = WVariable.sample_from(G)\n                    run_phase(\"W\", W_variable, config.global_lr_scale * 0.08)\n\n                    Wp_variable = WpVariable.from_W(W_variable)\n                    run_phase(\"W+\", Wp_variable, config.global_lr_scale * 0.02)\n\n                    Wpp_variable = WppVariable.from_Wp(Wp_variable)\n                    run_phase(\"W++\", Wpp_variable, config.global_lr_scale * 0.005)\n\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T19:00:45.831029Z","iopub.execute_input":"2025-11-22T19:00:45.831329Z","iopub.status.idle":"2025-11-22T19:00:45.837990Z","shell.execute_reply.started":"2025-11-22T19:00:45.831301Z","shell.execute_reply":"2025-11-22T19:00:45.837168Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Read the original content from run.py\n# with open('/kaggle/working/robust-unsupervised/run.py', 'r') as f:\n#     run_py_content = f.read()\n\n# Read the original content from cli.py\nwith open('/kaggle/working/robust-unsupervised/cli.py', 'r') as f:\n    cli_py_content = f.read()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T19:01:06.125812Z","iopub.execute_input":"2025-11-22T19:01:06.126209Z","iopub.status.idle":"2025-11-22T19:01:06.130964Z","shell.execute_reply.started":"2025-11-22T19:01:06.126172Z","shell.execute_reply":"2025-11-22T19:01:06.130259Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# This cell combines the early stopping logic, the switch to the Adam optimizer,\n# and the addition of detailed metrics logging into a single, sequential update.\n\nimport re\nimport math\n\n# --- STEP 1: READ THE ORIGINAL, UNMODIFIED FILES ---\n# with open('/kaggle/working/robust-unsupervised/run.py', 'r') as f:\n#     run_py_content = f.read()\n\nwith open('/kaggle/working/robust-unsupervised/cli.py', 'r') as f:\n    cli_py_content = f.read()\n\n# --- STEP 2: DEFINE THE `project` FUNCTION WITH EARLY STOPPING ---\ndef get_project_with_early_stopping():\n    return \"\"\"\ndef project(\n    G,\n    target: torch.Tensor,\n    original_image: torch.Tensor,\n    *,\n    config: Config,\n    initial_w=None,\n    progress=None,\n):\n    # Phase I\n    w = initial_w if initial_w is not None else G.mapping.w_avg.clone()\n    w.requires_grad_(True)\n    optimizer = Adam([w], lr=config.lr_phase_1)\n    \n    best_loss_1 = math.inf\n    patience_counter_1 = 0\n\n    for step in range(config.n_steps_phase_1):\n        synth_images = G.synthesis(w, noise_mode=\"const\")\n        loss, loss_dict = loss_fn(synth_images, target, original_image)\n        optimizer.zero_grad(set_to_none=True)\n        loss.backward()\n        optimizer.step()\n        progress.update(1, **loss_dict)\n\n        if loss.item() < best_loss_1:\n            best_loss_1 = loss.item()\n            patience_counter_1 = 0\n        else:\n            patience_counter_1 += 1\n        \n        if patience_counter_1 >= config.patience:\n            print(f\"Stopping early in Phase 1 at step {step}.\")\n            break\n\n    # Phase II\n    w_plus = w.unsqueeze(1).repeat(1, G.num_ws, 1)\n    w_plus.requires_grad_(True)\n    optimizer = Adam([w_plus], lr=config.lr_phase_2)\n\n    best_loss_2 = math.inf\n    patience_counter_2 = 0\n\n    for step in range(config.n_steps_phase_2):\n        synth_images = G.synthesis(w_plus, noise_mode=\"const\")\n        loss, loss_dict = loss_fn(synth_images, target, original_image)\n        optimizer.zero_grad(set_to_none=True)\n        loss.backward()\n        optimizer.step()\n        progress.update(1, **loss_dict)\n\n        if loss.item() < best_loss_2:\n            best_loss_2 = loss.item()\n            patience_counter_2 = 0\n        else:\n            patience_counter_2 += 1\n        \n        if patience_counter_2 >= config.patience:\n            print(f\"Stopping early in Phase 2 at step {step}.\")\n            break\n\n    # Phase III\n    w_plus_plus = w_plus.unsqueeze(1).repeat(1, G.num_filters, 1, 1)\n    w_plus_plus.requires_grad_(True)\n    optimizer = Adam([w_plus_plus], lr=config.lr_phase_3)\n    \n    best_loss_3 = math.inf\n    patience_counter_3 = 0\n\n    for step in range(config.n_steps_phase_3):\n        synth_images = G.synthesis(w_plus_plus, noise_mode=\"const\")\n        loss, loss_dict = loss_fn(synth_images, target, original_image)\n        optimizer.zero_grad(set_to_none=True)\n        loss.backward()\n        optimizer.step()\n        progress.update(1, **loss_dict)\n        \n        if loss.item() < best_loss_3:\n            best_loss_3 = loss.item()\n            patience_counter_3 = 0\n        else:\n            patience_counter_3 += 1\n        \n        if patience_counter_3 >= config.patience:\n            print(f\"Stopping early in Phase 3 at step {step}.\")\n            break\n\n    return synth_images\n\"\"\"\n\n# --- STEP 3: APPLY ALL MODIFICATIONS SEQUENTIALLY TO `run.py` ---\n\n# 3.1: Replace the original `project` function with our new version.\n# This version already includes the Adam optimizer and the correct function signature.\nproject_function_string = get_project_with_early_stopping()\nupdated_run_py_content = re.sub(r\"def project\\\\(.*?\\\\):.*?(?=return synth_images)\", project_function_string.strip(), run_py_content, flags=re.S)\n\n# 3.2: Add all necessary imports at the top of the file.\nimports_to_add = \"\"\"\nimport math\nfrom torch.optim import Adam\nfrom metrics import calculate_accuracy_lpips, calculate_realism_fid, calculate_fidelity_lpips\nimport shutil\n\"\"\"\nupdated_run_py_content = re.sub(r'(from robust_unsupervised.prelude import \\*)', r'\\\\1' + imports_to_add, updated_run_py_content)\n\n\n# 3.3: Update the main loop to pass the original image to the project function.\nupdated_run_py_content = updated_run_py_content.replace(\n    'projected_w = project(G, target=target, progress=progress)',\n    'projected_w = project(G, target=target, original_image=batch[\"image\"], progress=progress)'\n)\n\n\n# 3.4: Add the metrics calculation logic to the end of the `run` function.\nmetrics_logging_code = \"\"\"\n    # --- DETAILED METRICS CALCULATION ---\n    ground_truth_dir = config.dataset_path\n    restored_dir = os.path.join(task_dir, os.path.basename(config.dataset_path))\n    degraded_dir = task_dir\n    \n    accuracy = calculate_accuracy_lpips(restored_dir, ground_truth_dir)\n    realism = calculate_realism_fid(restored_dir, ground_truth_dir)\n    fidelity = calculate_fidelity_lpips(restored_dir, degraded_dir, degradation.f)\n\n    shutil.rmtree(restored_dir)\n\n    summary = f\\\"\\\"\\\"\n    -----------------------------------------------------\n    PERFORMANCE REPORT FOR TASK: {task_name}\n    -----------------------------------------------------\n    - Accuracy (LPIPS ↓): {accuracy:.4f}\n    - Realism (FID ↓):    {realism:.2f}\n    - Fidelity (LPIPS ↓): {fidelity:.4f}\n    -----------------------------------------------------\n    \\\"\\\"\\\"\n    print(summary)\n    with open(os.path.join(task_dir, 'performance_report.txt'), 'w') as f:\n        f.write(summary)\n\"\"\"\nupdated_run_py_content = re.sub(r'(if __name__ == \"__main__\":)', metrics_logging_code + r'\\\\n\\\\1', updated_run_py_content, flags=re.S)\n\n\n# --- STEP 4: APPLY MODIFICATIONS TO `cli.py` ---\n# Add the 'patience' parameter for the early stopping feature.\nupdated_cli_py_content = cli_py_content.replace(\n    '    n_steps_phase_3: int = 150',\n    '    n_steps_phase_3: int = 150\\\\n    patience: int = 15  # Steps to wait for improvement before stopping'\n)\n\n\n# --- STEP 5: WRITE THE FINAL, MODIFIED CONTENT TO THE FILES ---\nwith open('/kaggle/working/robust-unsupervised/run.py', 'w') as f:\n    f.write(updated_run_py_content)\nprint(\"✅ run.py updated with early stopping, Adam optimizer, and metrics logging.\")\n\nwith open('/kaggle/working/robust-unsupervised/cli.py', 'w') as f:\n    f.write(updated_cli_py_content)\nprint(\"✅ cli.py updated with 'patience' parameter for early stopping.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T19:01:08.859505Z","iopub.execute_input":"2025-11-22T19:01:08.860204Z","iopub.status.idle":"2025-11-22T19:01:08.870627Z","shell.execute_reply.started":"2025-11-22T19:01:08.860180Z","shell.execute_reply":"2025-11-22T19:01:08.870073Z"}},"outputs":[{"name":"stdout","text":"✅ run.py updated with early stopping, Adam optimizer, and metrics logging.\n✅ cli.py updated with 'patience' parameter for early stopping.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!pip install deepface","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T19:01:14.840357Z","iopub.execute_input":"2025-11-22T19:01:14.840607Z","iopub.status.idle":"2025-11-22T19:01:19.705783Z","shell.execute_reply.started":"2025-11-22T19:01:14.840589Z","shell.execute_reply":"2025-11-22T19:01:19.705064Z"}},"outputs":[{"name":"stdout","text":"Collecting deepface\n  Downloading deepface-0.0.95-py3-none-any.whl.metadata (35 kB)\nRequirement already satisfied: requests>=2.27.1 in /usr/local/lib/python3.11/dist-packages (from deepface) (2.32.4)\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (1.26.4)\nRequirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.11/dist-packages (from deepface) (2.2.3)\nRequirement already satisfied: gdown>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from deepface) (5.2.0)\nRequirement already satisfied: tqdm>=4.30.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (4.67.1)\nRequirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (11.2.1)\nRequirement already satisfied: opencv-python>=4.5.5.64 in /usr/local/lib/python3.11/dist-packages (from deepface) (4.11.0.86)\nRequirement already satisfied: tensorflow>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (2.18.0)\nRequirement already satisfied: keras>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (3.8.0)\nRequirement already satisfied: Flask>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from deepface) (3.1.1)\nCollecting flask-cors>=4.0.1 (from deepface)\n  Downloading flask_cors-6.0.1-py3-none-any.whl.metadata (5.3 kB)\nCollecting mtcnn>=0.1.0 (from deepface)\n  Downloading mtcnn-1.0.0-py3-none-any.whl.metadata (5.8 kB)\nCollecting retina-face>=0.0.14 (from deepface)\n  Downloading retina_face-0.0.17-py3-none-any.whl.metadata (10 kB)\nCollecting fire>=0.4.0 (from deepface)\n  Downloading fire-0.7.1-py3-none-any.whl.metadata (5.8 kB)\nCollecting gunicorn>=20.1.0 (from deepface)\n  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\nRequirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire>=0.4.0->deepface) (3.1.0)\nRequirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (1.9.0)\nRequirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (8.2.1)\nRequirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (2.2.0)\nRequirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (3.1.6)\nRequirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (3.0.2)\nRequirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (3.1.3)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=3.10.1->deepface) (4.13.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown>=3.10.1->deepface) (3.18.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gunicorn>=20.1.0->deepface) (25.0)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (1.4.0)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (14.0.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.1.0)\nRequirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (3.14.0)\nRequirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.16.0)\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.4.1)\nRequirement already satisfied: joblib>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from mtcnn>=0.1.0->deepface) (1.5.1)\nCollecting lz4>=4.3.3 (from mtcnn>=0.1.0->deepface)\n  Downloading lz4-4.4.5-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->deepface) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->deepface) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->deepface) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->deepface) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->deepface) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->deepface) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.4->deepface) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.4->deepface) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.4->deepface) (2025.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->deepface) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->deepface) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->deepface) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->deepface) (2025.6.15)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (25.2.10)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (18.1.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (3.4.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (3.20.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (75.2.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.17.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (4.14.0)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.17.2)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.73.1)\nRequirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (2.18.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.37.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow>=1.9.0->deepface) (0.45.1)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.9.0->deepface) (3.8.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.9.0->deepface) (0.7.2)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=3.10.1->deepface) (2.7)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->deepface) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->deepface) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->deepface) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.14.0->deepface) (2024.2.0)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (1.7.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=2.2.0->deepface) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=2.2.0->deepface) (2.19.2)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.14.0->deepface) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=2.2.0->deepface) (0.1.2)\nDownloading deepface-0.0.95-py3-none-any.whl (128 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.3/128.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fire-0.7.1-py3-none-any.whl (115 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.9/115.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading flask_cors-6.0.1-py3-none-any.whl (13 kB)\nDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mtcnn-1.0.0-py3-none-any.whl (1.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading retina_face-0.0.17-py3-none-any.whl (25 kB)\nDownloading lz4-4.4.5-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: lz4, gunicorn, fire, mtcnn, flask-cors, retina-face, deepface\nSuccessfully installed deepface-0.0.95 fire-0.7.1 flask-cors-6.0.1 gunicorn-23.0.0 lz4-4.4.5 mtcnn-1.0.0 retina-face-0.0.17\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# This code will completely overwrite your loss_function.py file\n\nnew_loss_function_content = \"\"\"\nfrom robust_unsupervised.prelude import *\nfrom lpips import LPIPS\nfrom deepface import DeepFace\n\n# Helper function to preprocess images for the face recognition model\ndef preprocess_for_face_recognition(tensor_image):\n    # The model expects images in a specific format (BGR, specific size etc.)\n    # We convert our PyTorch tensor to a NumPy array that deepface can use.\n    # The tensor is expected to be in range [-1, 1], so we shift to [0, 255]\n    image_np = (tensor_image.permute(0, 2, 3, 1) * 127.5 + 127.5).clamp(0, 255).to(torch.uint8).cpu().numpy()\n    # DeepFace handles the rest of the preprocessing internally\n    return image_np\n\nclass IDLoss(nn.Module):\n    def __init__(self):\n        super(IDLoss, self).__init__()\n        # Load the ArcFace model. It will be downloaded automatically the first time.\n        # We only need the model for embedding extraction.\n        self.model = DeepFace.build_model('ArcFace')\n        self.model.eval()\n        for param in self.model.parameters():\n            param.requires_grad = False\n\n    def forward(self, x, y):\n        # x and y are the generated and ground-truth images, respectively.\n        # Both are PyTorch tensors.\n        \n        # Preprocess images for the model\n        x_np = preprocess_for_face_recognition(x)\n        y_np = preprocess_for_face_recognition(y)\n\n        # Get the embeddings. DeepFace expects a list of images.\n        x_embedding = DeepFace.represent(img_path=x_np[0], model=self.model, enforce_detection=False)\n        y_embedding = DeepFace.represent(img_path=y_np[0], model=self.model, enforce_detection=False)\n        \n        # Convert embeddings back to PyTorch tensors\n        x_embedding_tensor = torch.tensor(x_embedding[0]['embedding']).unsqueeze(0).to(x.device)\n        y_embedding_tensor = torch.tensor(y_embedding[0]['embedding']).unsqueeze(0).to(y.device)\n\n        # Calculate Cosine Similarity Loss\n        loss = 1 - torch.cosine_similarity(x_embedding_tensor, y_embedding_tensor)\n        return loss.mean()\n\nclass LossFunction(nn.Module):\n    def __init__(self, lambda_l1: float = 0.1, lambda_id: float = 0.1):\n        super().__init__()\n        self.lambda_l1 = lambda_l1\n        self.lambda_id = lambda_id\n        self.l1 = nn.L1Loss()\n        self.lpips = LPIPS(net=\"vgg\").cuda()\n        self.id_loss = IDLoss().cuda()\n\n    def forward(self, synth_images, target_images, original_images):\n        # The main forward pass now accepts the original image\n        \n        # L1 Loss (Pixel-wise)\n        l1_loss = self.l1(synth_images, target_images)\n\n        # LPIPS Loss (Perceptual)\n        lpips_loss = self.lpips(synth_images, target_images).mean()\n        \n        # Identity Loss\n        id_loss = self.id_loss(synth_images, original_images)\n\n        # Combine the losses\n        total_loss = lpips_loss + self.lambda_l1 * l1_loss + self.lambda_id * id_loss\n        \n        return total_loss, {\n            \"loss\": total_loss.item(),\n            \"lpips\": lpips_loss.item(),\n            \"l1\": l1_loss.item(),\n            \"id\": id_loss.item(),\n        }\n\n# Global instance\nloss_fn = LossFunction()\n\"\"\"\n\n# Write the new content to the file\nwith open('/kaggle/working/robust-unsupervised/loss_function.py', 'w') as f:\n    f.write(new_loss_function_content)\n\nprint(\"✅ loss_function.py has been updated with the Identity Loss.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T19:01:33.859207Z","iopub.execute_input":"2025-11-22T19:01:33.859787Z","iopub.status.idle":"2025-11-22T19:01:33.865992Z","shell.execute_reply.started":"2025-11-22T19:01:33.859757Z","shell.execute_reply":"2025-11-22T19:01:33.865218Z"}},"outputs":[{"name":"stdout","text":"✅ loss_function.py has been updated with the Identity Loss.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# This cell creates a new file, 'metrics.py', to handle performance evaluation.\n# We will calculate Accuracy (LPIPS), Realism (FID), and Fidelity (LPIPS on re-degraded images).\n\nmetrics_py_content = \"\"\"\nfrom robust_unsupervised.prelude import *\nfrom pytorch_fid.fid_score import calculate_fid_given_paths\nfrom lpips import LPIPS\nimport os\n\n# Initialize the LPIPS model once to be reused.\nlpips_fn = LPIPS(net=\"vgg\").cuda()\n\ndef calculate_accuracy_lpips(restored_dir: str, ground_truth_dir: str) -> float:\n    \\\"\\\"\\\"\n    Calculates Accuracy, defined as the average LPIPS between restored\n    images and their ground truth counterparts. \n    \\\"\\\"\\\"\n    print(\"Calculating Accuracy (LPIPS)...\")\n    restored_files = sorted([os.path.join(restored_dir, f) for f in os.listdir(restored_dir)])\n    gt_files = sorted([os.path.join(ground_truth_dir, f) for f in os.listdir(ground_truth_dir)])\n    \n    total_lpips = 0.0\n    num_images = len(restored_files)\n    \n    for restored_path, gt_path in zip(restored_files, gt_files):\n        restored_img = read_image_tensor(restored_path).cuda()\n        gt_img = read_image_tensor(gt_path).cuda()\n        lpips_score = lpips_fn(restored_img, gt_img).item()\n        total_lpips += lpips_score\n        \n    return total_lpips / num_images if num_images > 0 else 0.0\n\ndef calculate_realism_fid(restored_dir: str, ground_truth_dir: str) -> float:\n    \\\"\\\"\\\"\n    Calculates Realism using Frechet Inception Distance (FID) between the set of \n    restored images and the set of ground truth images. [cite: 226]\n    The paper uses a patch-based FID (pFID), but we use standard FID as a strong proxy.\n    \\\"\\\"\\\"\n    print(\"Calculating Realism (FID)...\")\n    # These parameters are commonly used for FID calculation.\n    dims = 2048\n    batch_size = 32\n    device = torch.device(\"cuda\")\n    \n    fid_value = calculate_fid_given_paths(\n        paths=[restored_dir, ground_truth_dir],\n        batch_size=batch_size,\n        device=device,\n        dims=dims\n    )\n    return fid_value\n    \ndef calculate_fidelity_lpips(restored_dir: str, degraded_dir: str, degradation_fn) -> float:\n    \\\"\\\"\\\"\n    Calculates Fidelity, defined as the average LPIPS between a re-degraded\n    restored image and the original degraded target image. [cite: 228]\n    \\\"\\\"\\\"\n    print(\"Calculating Fidelity (LPIPS)...\")\n    restored_files = sorted([os.path.join(restored_dir, f) for f in os.listdir(restored_dir)])\n    degraded_files = sorted([os.path.join(degraded_dir, f) for f in os.listdir(degraded_dir)])\n\n    total_lpips = 0.0\n    num_images = len(restored_files)\n\n    for restored_path, degraded_path in zip(restored_files, degraded_files):\n        restored_img = read_image_tensor(restored_path).cuda()\n        degraded_target_img = read_image_tensor(degraded_path).cuda()\n        \n        # Apply the same degradation to our restored image. [cite: 229]\n        re_degraded_img = degradation_fn(restored_img)\n        \n        lpips_score = lpips_fn(re_degraded_img, degraded_target_img).item()\n        total_lpips += lpips_score\n\n    return total_lpips / num_images if num_images > 0 else 0.0\n\ndef read_image_tensor(path: str) -> torch.Tensor:\n    \\\"\\\"\\\"Helper to read an image and convert it to a PyTorch tensor in [-1, 1] range.\\\"\\\"\\\"\n    img = Image.open(path).convert(\"RGB\")\n    img_tensor = F.to_tensor(img) * 2 - 1\n    return img_tensor.unsqueeze(0)\n\"\"\"\n\n# Write the new content to the file\nwith open('/kaggle/working/robust-unsupervised/metrics.py', 'w') as f:\n    f.write(metrics_py_content)\n\nprint(\"✅ metrics.py created successfully with functions for Accuracy, Realism, and Fidelity.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T19:01:39.286684Z","iopub.execute_input":"2025-11-22T19:01:39.287389Z","iopub.status.idle":"2025-11-22T19:01:39.293814Z","shell.execute_reply.started":"2025-11-22T19:01:39.287363Z","shell.execute_reply":"2025-11-22T19:01:39.293061Z"}},"outputs":[{"name":"stdout","text":"✅ metrics.py created successfully with functions for Accuracy, Realism, and Fidelity.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"%cd /kaggle/working/robust-unsupervised\n\n!python run.py --dataset_path datasets/samples","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T19:01:44.062587Z","iopub.execute_input":"2025-11-22T19:01:44.062861Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/robust-unsupervised\nrestored_samples\nLoading generator from pretrained_networks/ffhq.pkl...\nDownloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|█████████████████████████████████████████| 528M/528M [00:02<00:00, 201MB/s]\nout/restored_samples/2025-11-22T190148/single_tasks/upsampling/XL/\n/kaggle/working/robust-unsupervised/out/restored_samples/2025-11-22T190148/single_tasks/upsampling/XL/datasets/samples\n- 0000\nW:   0%|                                                | 0/150 [00:00<?, ?it/s]Setting up PyTorch plugin \"bias_act_plugin\"... /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \nIf this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n  warnings.warn(\nDone.\nW: 100%|██████████████████████████████████████| 150/150 [00:42<00:00,  3.53it/s]\nW+: 100%|█████████████████████████████████████| 150/150 [00:20<00:00,  7.43it/s]\nW++: 100%|████████████████████████████████████| 150/150 [00:22<00:00,  6.57it/s]\n- 0001\nW: 100%|██████████████████████████████████████| 150/150 [00:20<00:00,  7.43it/s]\nW+: 100%|█████████████████████████████████████| 150/150 [00:20<00:00,  7.44it/s]\nW++: 100%|████████████████████████████████████| 150/150 [00:22<00:00,  6.59it/s]\nout/restored_samples/2025-11-22T190148/single_tasks/denoising/XL/\n/kaggle/working/robust-unsupervised/out/restored_samples/2025-11-22T190148/single_tasks/denoising/XL/datasets/samples\n- 0000\nW: 100%|██████████████████████████████████████| 150/150 [01:18<00:00,  1.91it/s]\nW+: 100%|█████████████████████████████████████| 150/150 [01:18<00:00,  1.91it/s]\nW++: 100%|████████████████████████████████████| 150/150 [01:21<00:00,  1.85it/s]\n- 0001\nW: 100%|██████████████████████████████████████| 150/150 [01:18<00:00,  1.90it/s]\nW+: 100%|█████████████████████████████████████| 150/150 [01:18<00:00,  1.90it/s]\nW++: 100%|████████████████████████████████████| 150/150 [01:21<00:00,  1.85it/s]\nout/restored_samples/2025-11-22T190148/single_tasks/deartifacting/XL/\n/kaggle/working/robust-unsupervised/out/restored_samples/2025-11-22T190148/single_tasks/deartifacting/XL/datasets/samples\n- 0000\nW: 100%|██████████████████████████████████████| 150/150 [01:32<00:00,  1.62it/s]\nW+: 100%|█████████████████████████████████████| 150/150 [01:32<00:00,  1.62it/s]\nW++: 100%|████████████████████████████████████| 150/150 [01:35<00:00,  1.58it/s]\n- 0001\nW: 100%|██████████████████████████████████████| 150/150 [01:32<00:00,  1.62it/s]\nW+: 100%|█████████████████████████████████████| 150/150 [01:32<00:00,  1.62it/s]\nW++: 100%|████████████████████████████████████| 150/150 [01:34<00:00,  1.58it/s]\nout/restored_samples/2025-11-22T190148/single_tasks/inpainting/XL/\n/kaggle/working/robust-unsupervised/out/restored_samples/2025-11-22T190148/single_tasks/inpainting/XL/datasets/samples\n- 0000\nW: 100%|██████████████████████████████████████| 150/150 [01:18<00:00,  1.90it/s]\nW+: 100%|█████████████████████████████████████| 150/150 [01:18<00:00,  1.90it/s]\nW++: 100%|████████████████████████████████████| 150/150 [01:21<00:00,  1.84it/s]\n- 0001\nW: 100%|██████████████████████████████████████| 150/150 [01:18<00:00,  1.90it/s]\nW+: 100%|█████████████████████████████████████| 150/150 [01:18<00:00,  1.90it/s]\nW++: 100%|████████████████████████████████████| 150/150 [01:21<00:00,  1.84it/s]\nout/restored_samples/2025-11-22T190148/single_tasks/upsampling/L/\n/kaggle/working/robust-unsupervised/out/restored_samples/2025-11-22T190148/single_tasks/upsampling/L/datasets/samples\n- 0000\nW: 100%|██████████████████████████████████████| 150/150 [00:21<00:00,  7.11it/s]\nW+: 100%|█████████████████████████████████████| 150/150 [00:21<00:00,  7.11it/s]\nW++: 100%|████████████████████████████████████| 150/150 [00:23<00:00,  6.34it/s]\n- 0001\nW: 100%|██████████████████████████████████████| 150/150 [00:21<00:00,  7.11it/s]\nW+: 100%|█████████████████████████████████████| 150/150 [00:21<00:00,  7.11it/s]\nW++: 100%|████████████████████████████████████| 150/150 [00:23<00:00,  6.32it/s]\nout/restored_samples/2025-11-22T190148/single_tasks/denoising/L/\n/kaggle/working/robust-unsupervised/out/restored_samples/2025-11-22T190148/single_tasks/denoising/L/datasets/samples\n- 0000\nW: 100%|██████████████████████████████████████| 150/150 [01:18<00:00,  1.90it/s]\nW+: 100%|█████████████████████████████████████| 150/150 [01:18<00:00,  1.90it/s]\nW++: 100%|████████████████████████████████████| 150/150 [01:21<00:00,  1.84it/s]\n- 0001\nW: 100%|██████████████████████████████████████| 150/150 [01:18<00:00,  1.90it/s]\nW+: 100%|█████████████████████████████████████| 150/150 [01:18<00:00,  1.90it/s]\nW++: 100%|████████████████████████████████████| 150/150 [01:21<00:00,  1.84it/s]\nout/restored_samples/2025-11-22T190148/single_tasks/deartifacting/L/\n/kaggle/working/robust-unsupervised/out/restored_samples/2025-11-22T190148/single_tasks/deartifacting/L/datasets/samples\n- 0000\nW: 100%|██████████████████████████████████████| 150/150 [01:32<00:00,  1.62it/s]\nW+: 100%|█████████████████████████████████████| 150/150 [01:32<00:00,  1.62it/s]\nW++: 100%|████████████████████████████████████| 150/150 [01:35<00:00,  1.57it/s]\n- 0001\nW: 100%|██████████████████████████████████████| 150/150 [01:32<00:00,  1.62it/s]\nW+: 100%|█████████████████████████████████████| 150/150 [01:32<00:00,  1.62it/s]\nW++: 100%|████████████████████████████████████| 150/150 [01:35<00:00,  1.58it/s]\nout/restored_samples/2025-11-22T190148/single_tasks/inpainting/L/\n/kaggle/working/robust-unsupervised/out/restored_samples/2025-11-22T190148/single_tasks/inpainting/L/datasets/samples\n- 0000\nW: 100%|██████████████████████████████████████| 150/150 [01:18<00:00,  1.90it/s]\nW+: 100%|█████████████████████████████████████| 150/150 [01:18<00:00,  1.90it/s]\nW++: 100%|████████████████████████████████████| 150/150 [01:21<00:00,  1.84it/s]\n- 0001\nW: 100%|██████████████████████████████████████| 150/150 [01:18<00:00,  1.90it/s]\nW+: 100%|█████████████████████████████████████| 150/150 [01:18<00:00,  1.90it/s]\nW++: 100%|████████████████████████████████████| 150/150 [01:21<00:00,  1.84it/s]\nout/restored_samples/2025-11-22T190148/single_tasks/upsampling/M/\n/kaggle/working/robust-unsupervised/out/restored_samples/2025-11-22T190148/single_tasks/upsampling/M/datasets/samples\n- 0000\nW: 100%|██████████████████████████████████████| 150/150 [00:22<00:00,  6.66it/s]\nW+: 100%|█████████████████████████████████████| 150/150 [00:22<00:00,  6.66it/s]\nW++: 100%|████████████████████████████████████| 150/150 [00:25<00:00,  5.98it/s]\n- 0001\nW: 100%|██████████████████████████████████████| 150/150 [00:22<00:00,  6.66it/s]\nW+: 100%|█████████████████████████████████████| 150/150 [00:22<00:00,  6.66it/s]\nW++: 100%|████████████████████████████████████| 150/150 [00:25<00:00,  5.97it/s]\nout/restored_samples/2025-11-22T190148/single_tasks/denoising/M/\n/kaggle/working/robust-unsupervised/out/restored_samples/2025-11-22T190148/single_tasks/denoising/M/datasets/samples\n- 0000\nW: 100%|██████████████████████████████████████| 150/150 [01:18<00:00,  1.90it/s]\nW+: 100%|█████████████████████████████████████| 150/150 [01:18<00:00,  1.90it/s]\nW++: 100%|████████████████████████████████████| 150/150 [01:21<00:00,  1.84it/s]\n- 0001\nW: 100%|██████████████████████████████████████| 150/150 [01:18<00:00,  1.90it/s]\nW+: 100%|█████████████████████████████████████| 150/150 [01:18<00:00,  1.90it/s]\nW++: 100%|████████████████████████████████████| 150/150 [01:21<00:00,  1.84it/s]\nout/restored_samples/2025-11-22T190148/single_tasks/deartifacting/M/\n/kaggle/working/robust-unsupervised/out/restored_samples/2025-11-22T190148/single_tasks/deartifacting/M/datasets/samples\n- 0000\nW: 100%|██████████████████████████████████████| 150/150 [01:32<00:00,  1.62it/s]\nW+: 100%|█████████████████████████████████████| 150/150 [01:32<00:00,  1.62it/s]\nW++:  45%|████████████████▌                    | 67/150 [00:35<00:43,  1.90it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}